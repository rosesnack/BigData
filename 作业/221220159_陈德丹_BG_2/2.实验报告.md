# 实验2 文档倒排索引
## 一(1)、带词频索引的文档倒排算法
#### map类
首先对输入键值对的文件来源作处理，如果是来自停用词文件则直接返回；之后将输入`value`对应的一行进行分词，对于每个词语，将词语作为`key`，文档名作为`value`并输出（类型均为`Text`）
```java
public static class InvertedIndexMapper extends Mapper<Object, Text, Text, Text> {
    @Override
    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        // default RecordReader: LineRecordReader; key: line offset; value: line string
        FileSplit fileSplit = (FileSplit) context.getInputSplit();
        String fileName = fileSplit.getPath().getName();
        if (fileName.equals("cn_stopwords.txt"))  return;

        Text word = new Text();
        StringTokenizer itr = new StringTokenizer(value.toString());
        for (; itr.hasMoreTokens(); ) {
            word.set(itr.nextToken());
            context.write(word, new Text(fileName));  // 词语作为key，文档名作为value
        }
    }
}
```
</div>
</div>

#### reduce类
建立一个`HashMap`用于存放所有出现过该词语的文档名和次数，在此过程中统计词语在所有文档中出现的总次数，以及包含该词语的文档数，从而计算出平均出现次数；然后再将`HashMap`遍历一遍，将每个文档对应的词频加在后面，从而输出符合要求的结果（`key`和`value`的类型均为`Text`）
```java
public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
    Iterator<Text> it = values.iterator();
    HashMap<String, Integer> countMap = new HashMap<>();  // {文档名，词语出现次数}
    int sum = 0;  // 词语在所有文档中出现的总次数
    int num = 0;  // 包含该词语的文档数
    while (it.hasNext()) {
        sum++;
        String docName = it.next().toString();
        if (countMap.containsKey(docName)) {
            countMap.put(docName, countMap.get(docName) + 1);  // 词语在该文档中出现的次数加1
        }
        else {
            countMap.put(docName,1);  // 词语在该文档中第一次出现
            num++;  // 包含该词语的文档数加1
        }
    }

    StringBuilder all = new StringBuilder();
    float avg = (float) sum / num; // 平均出现次数
    String formatted = String.format("%.2f", avg); // 保留两位小数
    all.append(formatted).append(", ");

    for (Map.Entry<String, Integer> entry : countMap.entrySet()) {//遍历文档名和该文档内的词语出现次数
        String docName = entry.getKey();
        int count = entry.getValue();
        all.append(docName).append(":").append(count).append("; ");  // 每个文档的记录
    }

    // 删除最后一个多余的分号和空格
    if (!countMap.isEmpty()) {
        all.setLength(all.length() - 2);
    }

    String docName = "[" + key.toString() + "]";
    context.write(new Text(docName), new Text(all.toString()));
}
```
</div>

### 运行结果
![alt text](image-14.png)

### 集群执行结果
输入命令`yarn jar BD2.jar InvertedIndexer /user/root/Exp2 output-Exp2/1-1`
输出结果存储在`/user/221220159stu/output-Exp2/1-1`
![alt text](image-18.png)
![alt text](image-19.png)
</div>

## 一(2)、对词语的平均出现次数进行全局排序
#### map类
输入的一行数据里，格式是`[词语] 平均出现次数, 所有文档词频`
现在使用正则表达将这三部分分开，把平均出现次数赋给`key`，剩余两部分合并赋给`value`，从而在`map`之后可以根据平均出现次数进行排序（`key`的类型为`FloatWritable`，`value`的类型为`Text`）
```java
public static class SortMapper extends Mapper<Object, Text, FloatWritable, Text>
{
    @Override
    public void map(Object key, Text value, Context context) throws IOException, InterruptedException
    {
        // parts[0] is the term, parts[1] is the average, parts[2] are the files
        String[] parts = value.toString().split(",|\\s+", 3);
        FloatWritable average = new FloatWritable(Float.parseFloat(parts[1]));
        Text term = new Text(parts[0] + "," + parts[2]);
        context.write(average, term);
    }
}
```
</div>

#### reduce类
把三部分还原成一开始的格式，即`key`是[词语]，`value`是平均出现次数+所有文档词频，并输出（`key`和`value`的类型均为`Text`）
```java
public static class SortReducer extends Reducer<FloatWritable, Text, Text, Text>
{
    @Override
    public void reduce(FloatWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException
    {
        for (Text value : values)
        {
            String[] parts = value.toString().split(",", 2);
            String new_key = parts[0];
            String new_value = key + ", " + parts[1];
            context.write(new Text(new_key), new Text(new_value));
        }
    }
}
```
</div>

#### DescendingFloatComparator类
`mapreduce`给`key`默认的排序是从小到大，但直观上我们更希望看到词频从大到小排序，因此可以自定义一个`float`的比较运算符，将已有的比较结果取反即可
```java
public static class DescendingFloatComparator extends FloatWritable.Comparator
{
    @Override
    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
    {
        return -super.compare(b1, s1, l1, b2, s2, l2);
    }
    @Override
    public int compare(Object a, Object b)
    {
        return -super.compare((WritableComparable)a, (WritableComparable)b);
    }
}
```
</div>

### 运行结果
![alt text](image-17.png)
</div>

### 集群执行结果
输入命令`yarn jar BD2.jar Sort /user/221220159stu/output-Exp2/1-1 output-Exp2/1-2`
输出结果存储在`/user/221220159stu/output-Exp2/1-2`
![alt text](image-15.png)
![alt text](image-16.png)
</div>

## 二、计算TF-IDF
在`main`函数中增加一个变量存储语料库里的文档数目
```java
Path inputPath = new Path(args[0]);
FileSystem fs = inputPath.getFileSystem(conf);
FileStatus[] fileStatuses = fs.listStatus(inputPath);
int fileCount = fileStatuses.length - 1; //去除停用词文档
conf.setInt("fileCount", fileCount);
fs.close();
```
#### map类
和`InvertedIndexer`基本一致，将输入`value`对应的一行进行分词，对于每个词语，将词语作为`key`，文档名作为`value`并输出（类型均为`Text`）
```java
public static class TF_IDFMapper extends Mapper<Object, Text, Text, Text> {
    @Override
    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        // default RecordReader: LineRecordReader; key: line offset; value: line string
        FileSplit fileSplit = (FileSplit) context.getInputSplit();
        String fileName = fileSplit.getPath().getName();
        if (fileName.equals("cn_stopwords.txt"))  return;
        Text word = new Text();
        StringTokenizer itr = new StringTokenizer(value.toString());
        for (; itr.hasMoreTokens(); ) {
            word.set(itr.nextToken());
            context.write(word, new Text(fileName));
        }
    }
}
```
</div>

#### reduce类
同样建立一个`HashMap`用于存放所有出现过该词语的文档名和次数，在此过程中统计包含该词的文档数；然后再将`HashMap`遍历一遍，为每个文档计算TF-IDF，并输出结果（`key`和`value`的类型均为`Text`）
在此过程中还出了bug，后来发现是`/`默认是整除，需要加上`(double)`才能得到非整数结果。
```java
public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
    Iterator<Text> it = values.iterator();
    HashMap<String, Integer> countMap = new HashMap<>();  // {文档名，词语出现次数}
    int num = 0;  // 包含该词语的文档数
    while (it.hasNext()) {
        String docName = it.next().toString();
        if (countMap.containsKey(docName)) {
            countMap.put(docName, countMap.get(docName) + 1);
        }
        else {
            countMap.put(docName,1);
            num++;
        }
    }

    Configuration conf = context.getConfiguration();
    int fileCount = conf.getInt("fileCount", 0);
    double IDF = Math.log((double)fileCount / (num + 1)) / Math.log(2);

    // 为每个文档计算TF-IDF
    for (Map.Entry<String, Integer> entry : countMap.entrySet()) {
        String docName = entry.getKey();
        int TF = entry.getValue();
        double TF_IDF = TF * IDF;
        Text value = new Text(String.format("%.2f, ", TF_IDF));
        context.write(new Text(docName + "[" + key.toString() + "]"), value);
    }
}
```
</div>

### 运行结果
![alt text](image-13.png)
</div>

### 集群执行结果
输入命令`yarn jar BD2.jar TF_IDF /user/root/Exp2 output-Exp2/2-2`
输出结果存储在`/user/221220159stu/output-Exp2/2-2`
![alt text](image-20.png)
![alt text](image-21.png)
</div>

## 三、去除停用词，输出排序后的结果
仿照`WordCount2.0`，输入时需新加一个参数表示停用词文档地址，
在`main`函数里新加一句`job.addCacheFile(new Path(args[2]).toUri());`
</div>

在`Mapper`的`setup`阶段读取停用词文档，并将停用词全都加到一个集合`patternsToSkip`里
```java
private Set<String> patternsToSkip = new HashSet<String>();
private BufferedReader fis;

public void setup(Context context) throws IOException, InterruptedException {
    //String fileName = "/home/nightheron/dataset/cn_stopwords.txt";
    Configuration conf = context.getConfiguration();
    URI[] patternsURIs = Job.getInstance(conf).getCacheFiles();
    for (URI patternsURI : patternsURIs) {
        Path patternsPath = new Path(patternsURI.getPath());
        String fileName = patternsPath.getName().toString();
        fis = new BufferedReader(new FileReader(fileName));
        String pattern = null;
        while ((pattern = fis.readLine()) != null) {
            patternsToSkip.add(pattern);
        }
    }
}
```
之后在`map`函数里，只有词语不在停用词集合里才写入
```java
if (!patternsToSkip.contains(word.toString())) {
    context.write(word, new Text(fileName));
}
```
`Reducer`与之前一致
运行完成后，还需要再通过之前完成的`Sort`进行分类。
</div>

### 运行结果
![alt text](image-9.png)
</div>

### 集群执行结果
依次输入命令①`yarn jar BD2.jar StopWords /user/root/Exp2 output-Exp2/3-1 hdfs://hcdsj/user/root/Exp2/cn_stopwords.txt`
②`yarn jar BD2.jar Sort /user/221220159stu/output-Exp2/3-1 output-Exp2/3-2`
输出结果存储在`/user/221220159stu/output-Exp2/3-2`
![alt text](image-10.png)
![alt text](image-11.png)
![alt text](image-12.png)
</div>